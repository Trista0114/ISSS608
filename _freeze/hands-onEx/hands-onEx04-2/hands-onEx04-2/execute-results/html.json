{
  "hash": "c45250fe8b01938a302107e8f5107935",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Hands-on Exercise 04-2: Visual Statistical Analysis\"\nauthor: \"Chen.PengWei\"\ndate-modified: \"last-modified\"\nexecute:\n  echo: true\n  eval: true\n  warning: false\n  freeze: true\n---\n\n\n\n# 1. Learning Outcome\n\n------------------------------------------------------------------------\n\nIn this hands-on exercise, you will gain hands-on experience on using:\n\n-   ggstatsplot package to create visual graphics with rich **statistical information**,\n\n-   performance package to visualise **model diagnostics**, and\n\n-   parameters package to **visualise model parameters**\n\n# 2. Visual Statistical Analysis with ggstatsplot\n\n------------------------------------------------------------------------\n\n[**ggstatsplot**](https://indrajeetpatil.github.io/ggstatsplot/index.html)¬†![](https://r4va.netlify.app/chap10/img/image1.jpg){width=\"36\" height=\"41\"}¬†is an extension of¬†[**ggplot2**](https://ggplot2.tidyverse.org/)¬†package for creating graphics with details from statistical tests included in the information-rich plots themselves.\n\nFor all statistical tests reported in the plots, the default template abides by the \\[APA\\](https://my.ilstu.edu/\\~jhkahn/apastats.html) gold standard for statistical reporting. For example, here are results from a robust t-test:\n\n![](images/image9.jpg)\n\n# 3. Getting Started\n\n::: panel-tabset\n## Installing libraries\n\nIn this exercise,¬†**ggstatsplot**¬†and¬†**tidyverse**¬†will be used.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(ggstatsplot, tidyverse)\n```\n:::\n\n\n\n## Importing data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nExam_data <- read_csv(\"C:/Trista0114/ISSS608/hands-onEx/hands-onEx01/Exam_data.csv\")\n```\n:::\n\n\n\n## Understanding data\n\nThe detail of attribute description is also show in the previous exercise.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nExam_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 322 √ó 7\n   ID         CLASS GENDER RACE    ENGLISH MATHS SCIENCE\n   <chr>      <chr> <chr>  <chr>     <dbl> <dbl>   <dbl>\n 1 Student321 3I    Male   Malay        21     9      15\n 2 Student305 3I    Female Malay        24    22      16\n 3 Student289 3H    Male   Chinese      26    16      16\n 4 Student227 3F    Male   Chinese      27    77      31\n 5 Student318 3I    Male   Malay        27    11      25\n 6 Student306 3I    Female Malay        31    16      16\n 7 Student313 3I    Male   Chinese      31    21      25\n 8 Student316 3I    Male   Malay        31    18      27\n 9 Student312 3I    Male   Malay        33    19      15\n10 Student297 3H    Male   Indian       34    49      37\n# ‚Ñπ 312 more rows\n```\n\n\n:::\n:::\n\n\n:::\n\n# 4. Stat test by using ggstatsplot\n\n------------------------------------------------------------------------\n\n## 4.1 One-sample test\n\n[*gghistostats()*](https://indrajeetpatil.github.io/ggstatsplot/reference/gghistostats.html)¬†is used to to build an visual of one-sample test on English scores.\n\nPerforms a Bayesian one-sample test to see if the mean differs from 60, and displays the test results and other statistical information on the chart.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\ngghistostats(\n  data = Exam_data,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n```\n\n::: {.cell-output-display}\n![](hands-onEx04-2_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n## 4.2 Bayes Factor\n\n-   A Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\n\n-   That‚Äôs because the Bayes factor gives us a way to evaluate the data in favor of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favor of a given hypothesis.\n\n-   When we are comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes Factor is often written as B10. It can be defined mathematically as\n\n![](images/clipboard-3407754228.jpeg){width=\"576\"}\n\nThe [**Schwarz criterion**](https://www.statisticshowto.com/bayesian-information-criterion/) is one of the easiest ways to calculate rough approximation of the Bayes Factor.\n\nA¬†**Bayes Factor**¬†can be any positive number. One of the most common interpretations is this one‚Äîfirst proposed by Harold Jeffereys (1961) and slightly modified by¬†[Lee and Wagenmakers](https://www-tandfonline-com.libproxy.smu.edu.sg/doi/pdf/10.1080/00031305.1999.10474443?needAccess=true)¬†in 2013:\n\n![](images/image11.jpg){width=\"655\"}\n\n## 4.3 Two-sample mean test: *`ggbetweenstats()`*\n\n[*ggbetweenstats()*](https://indrajeetpatil.github.io/ggstatsplot/reference/ggbetweenstats.html)¬†is used to build a visual for two-sample mean test of Maths scores by gender.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggbetweenstats(\n  data = Exam_data,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",#a nonparametric test should be used.\n  messages = FALSE\n)\n```\n\n::: {.cell-output-display}\n![](hands-onEx04-2_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n## 4.4 Oneway ANOVA Test: *`ggbetweenstats()`*\n\n[*ggbetweenstats()*](https://indrajeetpatil.github.io/ggstatsplot/reference/ggbetweenstats.html)¬†is used to build a visual for One-way ANOVA test on English score by race.\n\n`pairwise.display = \"s\"`\n\n-   ‚Äúns‚Äù ‚Üí only non-significant\n-   ‚Äús‚Äù ‚Üí only significant\n-   ‚Äúall‚Äù ‚Üí everything\n\n`p.adjust.method = \"fdr\"`\n\n-   Sets the method to adjust p-values in multiple comparisons to ‚ÄúFDR‚Äù (False Discovery Rate), helping control for multiple testing errors.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggbetweenstats(\n  data = Exam_data,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\", #parametric test \n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, # additional pairwise tests (e.g., pairwise t-tests) between each pair of groups\n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n```\n\n::: {.cell-output-display}\n![](hands-onEx04-2_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n## 4.5 ggbetweenstats - Summary of tests\n\n![![](images/image13.jpg)](images/image12.jpg)\n\n![](images/image14.jpg)\n\nGenerally, when the data conforms to the normal distribution, use the parametric test; when the data is obviously skewed, choose the non-parametric or robust test.\n\n## 4.6 Significant Test of Correlation: *`ggscatterstats()`*\n\n[*ggscatterstats()*](https://indrajeetpatil.github.io/ggstatsplot/reference/ggscatterstats.html)¬†is used to build a visual for Significant Test of Correlation between Maths scores and English scores.\n\n-   There is a high positive correlation (r = 0.83) between mathematics scores and English scores, and the statistical significance is extremely strong ( ùëù ‚â™ 0.05 p‚â™0.05).\n-   Improvements in math scores are often accompanied by improvements in English scores.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggscatterstats(\n  data = Exam_data,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = FALSE,\n  )\n```\n\n::: {.cell-output-display}\n![](hands-onEx04-2_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n## 4.7 Significant Test of Association (Depedence) : *`ggbarstats()`* \n\nMaths scores is binned into a 4-class variable by using¬†[*cut()*](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/cut). [*ggbarstats()*](https://indrajeetpatil.github.io/ggstatsplot/reference/ggbarstats.html)¬†is used to build a visual for Significant Test of Association.\n\n-   There is no significant relationship between individuality and the distribution of mathematics scores (p = 0.79, Cram√©r's V ‚âà 0).\n-   The proportions of girls and boys in each performance range are similar, indicating that the correlation between mathematics performance and gender is low in this data set.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexam1 <- Exam_data %>% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,100))\n)\nggbarstats(exam1, \n           x = MATHS_bins, \n           y = GENDER)\n```\n\n::: {.cell-output-display}\n![](hands-onEx04-2_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n# 5. Visualising Models\n\n------------------------------------------------------------------------\n\nIn this section, you will learn how to visualise model diagnostic and model parameters by using parameters package.\n\n-   Toyota Corolla case study will be used. The purpose of study is to build a model to discover factors affecting prices of used-cars by taking into consideration a set of explanatory variables.\n\n## 5.1 Getting Started\n\n::: panel-tabset\n## Installing libraries\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(readxl, performance, parameters, see)\n```\n:::\n\n\n\n## Importing data\n\nNotice that the output object¬†`car_resale`¬†is a tibble data frame.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar_resale <- read_xls(\"C:/Trista0114/ISSS608/hands-onEx/hands-onEx04-2/ToyotaCorolla.xls\", \n                       \"data\")\ncar_resale\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,436 √ó 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   <dbl> <chr>    <dbl>     <dbl>     <dbl>    <dbl>  <dbl>         <dbl>  <dbl>\n 1    81 TOYOTA ‚Ä¶ 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA ‚Ä¶ 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA ‚Ä¶ 13750        23        10     2002  72937           210   1165\n 4     3 ¬†TOYOTA‚Ä¶ 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA ‚Ä¶ 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA ‚Ä¶ 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA ‚Ä¶ 12950        32         1     2002  61000           210   1170\n 8     7 ¬†TOYOTA‚Ä¶ 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA ‚Ä¶ 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA ‚Ä¶ 16950        27         6     2002 110404           234   1255\n# ‚Ñπ 1,426 more rows\n# ‚Ñπ 29 more variables: Guarantee_Period <dbl>, HP_Bin <chr>, CC_bin <chr>,\n#   Doors <dbl>, Gears <dbl>, Cylinders <dbl>, Fuel_Type <chr>, Color <chr>,\n#   Met_Color <dbl>, Automatic <dbl>, Mfr_Guarantee <dbl>,\n#   BOVAG_Guarantee <dbl>, ABS <dbl>, Airbag_1 <dbl>, Airbag_2 <dbl>,\n#   Airco <dbl>, Automatic_airco <dbl>, Boardcomputer <dbl>, CD_Player <dbl>,\n#   Central_Lock <dbl>, Powered_Windows <dbl>, Power_Steering <dbl>, ‚Ä¶\n```\n\n\n:::\n:::\n\n\n:::\n\n## 5.2 Multiple Regression Model: `lm()`\n\nThe code chunk below is used to calibrate a multiple linear regression model by using¬†*lm() to find the factors that influences the Price.*\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n```\n\n\n:::\n:::\n\n\n\n‚úÖ There is a negative correlation between vehicle **age (Age_08_04)** and **mileage (KM)**. The older the vehicle and the more it is driven, the lower the price.\\\n\\\n‚úÖ Manufacturing year **(Mfg_Year)**, weight **(Weight)** and warranty period **(Guarantee_Period)** are positively correlated. The newer, heavier the car, the longer the warranty period, the higher the price.\n\n## 5.3 Model Diagnostic: checking for multicolinearity\n\n`check_collinearity()`\n\nMulticollinearity occurs when two or more independent variables in a regression model are highly correlated, leading to instability in coefficient estimates and reducing interpretability.\n\n::: callout-note\n-   **VIF (Variance Inflation Factor)**: Measures how much the variance of a coefficient is inflated due to multicollinearity.\n\n    -   **VIF \\< 5** ‚Üí Low or acceptable correlation.\n\n    -   **VIF \\> 10** ‚Üí Severe multicollinearity (problematic).\n\n    -   **VIF \\> 30** ‚Üí Extremely high multicollinearity (must be addressed).\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_collinearity(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_c <- check_collinearity(model)\nplot(check_c)\n```\n\n::: {.cell-output-display}\n![](hands-onEx04-2_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\nPlease **remove one variable** to eliminate redundancy.\n\n## 5.4 Model Diagnostic: checking normality assumption\n\n*`check_normality()`*\n\n**The Q-Q plot** compares the distribution of residuals against a theoretical normal distribution. If residuals are normally distributed, the points should **fall along the diagonal line**.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel1 <- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_n <- check_normality(model1)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(check_n)\n```\n\n::: {.cell-output-display}\n![](hands-onEx04-2_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n::: callout-note\n**What impact will have on regression models?\\\n**If the residuals do not comply with normality, it may affect the accuracy of the p-value and confidence interval, making the hypothesis testing results unreliable. If the number of samples is n \\> 30, the regression results may still be valid according to the central limit theorem (CLT), but further checking is recommended.\n:::\n\n## 5.5 Model Diagnostic: Check model for homogeneity of variances\n\n*`check_heteroscedasticity()`*\n\nThe residuals should be **randomly scattered** around 0 with **no clear pattern**.\n\n-   Current plot observations:\n\n    -   Residuals increase in spread as fitted values increase.\n\n    -   The green trend line curves upward.\n\n    -   This suggests heteroscedasticity (non-constant variance).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_h <- check_heteroscedasticity(model1)\nplot(check_h)\n```\n\n::: {.cell-output-display}\n![](hands-onEx04-2_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\n::: callout-note\n**Solution:\\\nVariable transformation**: taking the log of the price (log transformation)\\\nWhen the variation increases with the predicted value, it usually means that the data has a \"multiplicative effect\". You can try log transformation Price to stabilize the variation.\n:::\n\n## 5.6 Model Diagnostic: Complete check\n\n`check_model`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_model(model1)\n```\n\n::: {.cell-output-display}\n![](hands-onEx04-2_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\n## 5.7 Visualising Regression Parameters: see methods\n\n**Red Points**: Negative coefficients (negative impact on `Price`).\\\n**Blue Points**: Positive coefficients (positive impact on `Price`).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(parameters(model1))\n```\n\n::: {.cell-output-display}\n![](hands-onEx04-2_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\n-   The confidence interval of `KM` might overlap with zero.\n\n-   The **negative effect of `Age_08_04`** makes sense, but it could be correlated with `Mfg_Year` (previously identified multicollinearity issue).\n\n-   **If a variable‚Äôs confidence interval crosses zero**, it might be **redundant and could be removed** from the model.\n\n## 5.8 Visualising Regression Parameters: *`ggcoefstats()`* \n\n*`ggcoefstats()`*\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggcoefstats(model1, \n            output = \"plot\")\n```\n\n::: {.cell-output-display}\n![](hands-onEx04-2_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n\n::: callout-note\n**AIC & BIC**\n\n-   AIC = 24,915, BIC = 24,946\n-   Lower values indicate a better model fit while penalizing complexity.\n-   These can be compared with alternative models to determine which regression model performs best.\n:::\n\n|   | Coefficient (Œ≤) | t-value | p-value | Significance |\n|---------------|---------------|---------------|---------------|---------------|\n| Guarantee_Period | 26.82 | 2.13 | 0.03 | ‚úÖ Significant (p \\< 0.05 |\n| Weight | 19.72 | 25.53 | 8.74e-104 | ‚úÖ Highly significant |\n| KM (Mileage) | -0.02 | -20.04 | 5.98e-79 | ‚úÖ Highly significant |\n| Age_08_04 (Car Age) | -119.49 | -43.29 | 2.10e-262 | ‚úÖ Highly significant |\n\n## 5.9 Conclusion\n\n\\\n**Why is the p-value of KM less than 0.05?**\\\nIn the graph (the result of 5.8 ggcoefstats()), the variable KM has a p-value of 5.98e-79 (very small), which means that it is statistically significant for Price. However, in the previous plot (5.7 plot(parameters(model1))), the coefficient plot for KM looks close to 0, which can lead to confusion.\n\n**Possible Reasons**\n\n1.  **Small coefficient but low variability can lead to a very small p-value**\n    -   If the variability of `KM` in the dataset is low (i.e., most values are concentrated in a narrow range), statistical tests may still detect `KM` as a stable and significant predictor, leading to a very small p-value.\n    -   This suggests that mileage has a consistent effect on price, but the effect size is small.\n2.  **Effect of Variable Units**\n    -   `KM` is likely measured in kilometers, and its values may be large (e.g., 10,000+ km).\n    -   For example, if a car has 100,000 km, the impact on price would be: 100,000√ó(‚àí0.02)=‚àí2,000100,000 \\times (-0.02) = -2,000100,000√ó(‚àí0.02)=‚àí2,000\n    -   This means driving an extra 100,000 km could reduce the price by 2,000, which is practically significant.\n3.  **Large Sample Size**\n    -   If the dataset contains a large number of observations (n is large), even tiny effects can be detected as statistically significant.\n\n::: callout-important\n`KM` has a very small p-value (statistically significant), but this does not necessarily mean it has a large impact on `Price`. **Practical interpretation is essential rather than relying solely on p-values.**\n:::\n\n## 5.10 Reference\n\nPart of the content of this pages were generated by ChatGPT and [Kam, T.S. (2023). Visual Statistical Analysis.](https://r4va.netlify.app/chap10#getting-started-1)\n",
    "supporting": [
      "hands-onEx04-2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}